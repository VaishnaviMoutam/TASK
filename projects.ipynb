{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9661e31-03d2-4728-acad-24be52dfaa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Accuracy: 1.00, Loss: 0.00\n",
      "Validation Accuracy: 1.00\n",
      "Error reading file evaluation_results_l2.txt\n",
      "Error reading file final_model_l2.h5\n",
      "Error reading file evaluation_results_dropout.txt\n",
      "Error reading file final_model_dropout.h5\n",
      "Error reading file final_model_l1.h5\n",
      "Error reading file evaluation_results_l1.txt\n",
      "Testing Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "#Implement a perceptron from scratch \n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Load the training data from the folder\n",
    "train_folder = '/home/vaishnavi-moutam/Downloads/final/Training1'\n",
    "X_train = []\n",
    "y_train = []\n",
    "for file in os.listdir(train_folder):\n",
    "    img = cv2.imread(os.path.join(train_folder, file))\n",
    "    if img is not None:  # Check if image is not empty\n",
    "        img = cv2.resize(img, (32, 32))  # Resize the image\n",
    "        img = img.flatten()  # Flatten the image into a 1D array\n",
    "        X_train.append(img)  # Append the flattened image to the list\n",
    "        y_train.append(1)  # or 0, depending on the class label\n",
    "    else:\n",
    "        print(f\"Error reading file {file}\")  # Print an error message if the image is empty\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Initialize the weights and bias\n",
    "weights = np.random.rand(X_train.shape[1])\n",
    "bias = 0\n",
    "\n",
    "# Set the learning rate and number of iterations\n",
    "learning_rate = 0.01\n",
    "n_iters = 1\n",
    "\n",
    "# Train the model\n",
    "for iter in range(n_iters):\n",
    "    for i, x_i in enumerate(X_train):\n",
    "        # Calculate the predicted output\n",
    "        predicted = np.where(np.dot(x_i, weights) + bias >= 0, 1, -1)\n",
    "        # Update the weights and bias\n",
    "        update = learning_rate * (y_train[i] - predicted)\n",
    "        weights += update * x_i\n",
    "        bias += update\n",
    "    # Calculate the accuracy and loss\n",
    "    accuracy = np.mean(predicted == y_train)\n",
    "    loss = np.mean((predicted - y_train) ** 2)\n",
    "    # Print the accuracy and loss at each iteration\n",
    "    print(f'Iteration {iter+1}, Accuracy: {accuracy:.2f}, Loss: {loss:.2f}')\n",
    "\n",
    "# Save the trained model\n",
    "np.save('weights.npy', weights)\n",
    "np.save('bias.npy', bias)\n",
    "\n",
    "# Load the validation data from the folder\n",
    "val_folder = '/home/vaishnavi-moutam/Downloads/final/Validation1'\n",
    "X_val = []\n",
    "y_val = []\n",
    "for file in os.listdir(val_folder):\n",
    "    img = cv2.imread(os.path.join(val_folder, file))\n",
    "    if img is not None:  # Check if image is not empty\n",
    "        img = cv2.resize(img, (32, 32))  # Resize the image\n",
    "        img = img.flatten()  # Flatten the image into a 1D array\n",
    "        X_val.append(img)  # Append the flattened image to the list\n",
    "        y_val.append(1)  # or 0, depending on the class label\n",
    "    else:\n",
    "        print(f\"Error reading file {file}\")  # Print an error message if the image is empty\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "# Load the trained model\n",
    "weights = np.load('weights.npy')\n",
    "bias = np.load('bias.npy')\n",
    "\n",
    "# Make predictions on the validation set\n",
    "predictions = np.where(np.dot(X_val, weights) + bias >= 0, 1, -1)\n",
    "# Calculate the accuracy on the validation set\n",
    "accuracy = np.mean(predictions == y_val)\n",
    "print(f'Validation Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Load the testing data from the folder\n",
    "test_folder = '/home/vaishnavi-moutam/Downloads/final/Testing1'\n",
    "X_test = []\n",
    "y_test = []\n",
    "for file in os.listdir(test_folder):\n",
    "    img = cv2.imread(os.path.join(test_folder, file))\n",
    "    if img is not None:  # Check if image is not empty\n",
    "        img = cv2.resize(img, (32, 32))  # Resize the image\n",
    "        img = img.flatten()  # Flatten the image into a 1D array\n",
    "        X_test.append(img)  # Append the flattened image to the list\n",
    "        y_test.append(1)  # or 0, depending on the class label\n",
    "    else:\n",
    "        print(f\"Error reading file {file}\")  # Print an error message if the image is empty\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Load the trained model\n",
    "weights = np.load('weights.npy')\n",
    "bias = np.load('bias.npy')\n",
    "\n",
    "# Make predictions on the testing set\n",
    "predictions = np.where(np.dot(X_test, weights) + bias >= 0, 1, -1)\n",
    "# Calculate the accuracy on the testing set\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f'Testing Accuracy: {accuracy:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec695271-4a8d-43d9-aea4-85d3dd654a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping /home/vaishnavi-moutam/Downloads/final/Testing1/evaluation_results_dropout.txt: cannot identify image file '/home/vaishnavi-moutam/Downloads/final/Testing1/evaluation_results_dropout.txt'\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Testing1/evaluation_results_l1.txt: cannot identify image file '/home/vaishnavi-moutam/Downloads/final/Testing1/evaluation_results_l1.txt'\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Testing1/evaluation_results_l2.txt: cannot identify image file '/home/vaishnavi-moutam/Downloads/final/Testing1/evaluation_results_l2.txt'\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Testing1/final_model_dropout.h5: cannot find loader for this HDF5 file\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Testing1/final_model_l1.h5: cannot find loader for this HDF5 file\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Testing1/final_model_l2.h5: cannot find loader for this HDF5 file\n",
      "Epoch [1/10], Loss: 0.4976, Train Acc: 1.0000, Val Acc: 1.0000\n",
      "Epoch [2/10], Loss: 0.0014, Train Acc: 1.0000, Val Acc: 1.0000\n",
      "Epoch [3/10], Loss: 0.0002, Train Acc: 1.0000, Val Acc: 1.0000\n",
      "Testing Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#Build and train a simple neural network using a framework like TensorFlow or PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "training_path = \"/home/vaishnavi-moutam/Downloads/final/Training1\"\n",
    "validation_path = \"/home/vaishnavi-moutam/Downloads/final/Validation1\"\n",
    "testing_path = \"/home/vaishnavi-moutam/Downloads/final/Testing1\"\n",
    "\n",
    "root = training_path\n",
    "transform = transform\n",
    "samples = []\n",
    "for root, _, fnames in sorted(os.walk(root)):\n",
    "    for fname in sorted(fnames):\n",
    "        path = os.path.join(root, fname)\n",
    "        try:\n",
    "            img = Image.open(path).convert('L')\n",
    "            samples.append((path, img))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {path}: {str(e)}\")\n",
    "\n",
    "train_loader = DataLoader([(transform(sample[1]), 0) for sample in samples], batch_size=32, shuffle=True)\n",
    "\n",
    "root = validation_path\n",
    "transform = transform\n",
    "samples = []\n",
    "for root, _, fnames in sorted(os.walk(root)):\n",
    "    for fname in sorted(fnames):\n",
    "        path = os.path.join(root, fname)\n",
    "        try:\n",
    "            img = Image.open(path).convert('L')\n",
    "            samples.append((path, img))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {path}: {str(e)}\")\n",
    "\n",
    "val_loader = DataLoader([(transform(sample[1]), 0) for sample in samples], batch_size=32, shuffle=False)\n",
    "\n",
    "root = testing_path\n",
    "transform = transform\n",
    "samples = []\n",
    "for root, _, fnames in sorted(os.walk(root)):\n",
    "    for fname in sorted(fnames):\n",
    "        path = os.path.join(root, fname)\n",
    "        try:\n",
    "            img = Image.open(path).convert('L')\n",
    "            samples.append((path, img))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {path}: {str(e)}\")\n",
    "\n",
    "test_loader = DataLoader([(transform(sample[1]), 0) for sample in samples], batch_size=32, shuffle=False)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28 * 28, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10)\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Calculate training accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in train_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # Calculate validation accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_acc = correct / total\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/10], Loss: {epoch_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "test_acc = correct / total\n",
    "print(f\"Testing Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c91c607-6cad-40ac-bf2f-760c60f7230c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /home/vaishnavi-moutam/Downloads/final/Training1/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/vaishnavi-moutam/Downloads/final/Training1/MNIST/raw/train-images-idx3-ubyte.gz to /home/vaishnavi-moutam/Downloads/final/Training1/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /home/vaishnavi-moutam/Downloads/final/Training1/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/vaishnavi-moutam/Downloads/final/Training1/MNIST/raw/train-labels-idx1-ubyte.gz to /home/vaishnavi-moutam/Downloads/final/Training1/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /home/vaishnavi-moutam/Downloads/final/Training1/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/vaishnavi-moutam/Downloads/final/Training1/MNIST/raw/t10k-images-idx3-ubyte.gz to /home/vaishnavi-moutam/Downloads/final/Training1/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /home/vaishnavi-moutam/Downloads/final/Training1/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/vaishnavi-moutam/Downloads/final/Training1/MNIST/raw/t10k-labels-idx1-ubyte.gz to /home/vaishnavi-moutam/Downloads/final/Training1/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /home/vaishnavi-moutam/Downloads/final/Validation1/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/vaishnavi-moutam/Downloads/final/Validation1/MNIST/raw/train-images-idx3-ubyte.gz to /home/vaishnavi-moutam/Downloads/final/Validation1/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /home/vaishnavi-moutam/Downloads/final/Validation1/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/vaishnavi-moutam/Downloads/final/Validation1/MNIST/raw/train-labels-idx1-ubyte.gz to /home/vaishnavi-moutam/Downloads/final/Validation1/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /home/vaishnavi-moutam/Downloads/final/Validation1/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/vaishnavi-moutam/Downloads/final/Validation1/MNIST/raw/t10k-images-idx3-ubyte.gz to /home/vaishnavi-moutam/Downloads/final/Validation1/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /home/vaishnavi-moutam/Downloads/final/Validation1/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/vaishnavi-moutam/Downloads/final/Validation1/MNIST/raw/t10k-labels-idx1-ubyte.gz to /home/vaishnavi-moutam/Downloads/final/Validation1/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /home/vaishnavi-moutam/Downloads/final/Testing1/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/vaishnavi-moutam/Downloads/final/Testing1/MNIST/raw/train-images-idx3-ubyte.gz to /home/vaishnavi-moutam/Downloads/final/Testing1/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /home/vaishnavi-moutam/Downloads/final/Testing1/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/vaishnavi-moutam/Downloads/final/Testing1/MNIST/raw/train-labels-idx1-ubyte.gz to /home/vaishnavi-moutam/Downloads/final/Testing1/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /home/vaishnavi-moutam/Downloads/final/Testing1/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/vaishnavi-moutam/Downloads/final/Testing1/MNIST/raw/t10k-images-idx3-ubyte.gz to /home/vaishnavi-moutam/Downloads/final/Testing1/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /home/vaishnavi-moutam/Downloads/final/Testing1/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/vaishnavi-moutam/Downloads/final/Testing1/MNIST/raw/t10k-labels-idx1-ubyte.gz to /home/vaishnavi-moutam/Downloads/final/Testing1/MNIST/raw\n",
      "\n",
      "Epoch 1, Loss: 0.4304706494432249\n",
      "Epoch 2, Loss: 0.17740139838212676\n",
      "Epoch 3, Loss: 0.129469789271397\n",
      "Epoch 4, Loss: 0.10660284879670072\n",
      "Epoch 5, Loss: 0.08746009690710915\n",
      "Epoch 6, Loss: 0.07567427212371429\n",
      "Epoch 7, Loss: 0.06496873360650024\n",
      "Epoch 8, Loss: 0.05941510789907411\n",
      "Epoch 9, Loss: 0.05250461398492932\n",
      "Epoch 10, Loss: 0.044819096597299185\n",
      "Training Accuracy: 98.76833333333333 %\n",
      "Test Accuracy: 97.52 %\n",
      "Validation Accuracy: 97.52 %\n"
     ]
    }
   ],
   "source": [
    "#Create a multi-layer perceptron (MLP) for digit classification (MNIST dataset)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define transforms for preprocessing\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load data from folders\n",
    "train_data = datasets.MNIST('/home/vaishnavi-moutam/Downloads/final/Training1', download=True, train=True, transform=transform)\n",
    "validation_data = datasets.MNIST('/home/vaishnavi-moutam/Downloads/final/Validation1', download=True, train=False, transform=transform)\n",
    "test_data = datasets.MNIST('/home/vaishnavi-moutam/Downloads/final/Testing1', download=True, train=False, transform=transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Multi-layer perceptron (MLP) model\n",
    "mlp = nn.Sequential(\n",
    "    nn.Linear(784, 128),  # input layer (28x28) -> hidden layer (128)\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),  # hidden layer (128) -> hidden layer (64)\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10)  # hidden layer (64) -> output layer (10)\n",
    ")\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(mlp.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 784)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/i}')\n",
    "\n",
    "# Evaluate on training set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 784)\n",
    "        outputs = mlp(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Training Accuracy: {100 * correct / total} %')\n",
    "\n",
    "# Evaluate on test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 784)\n",
    "        outputs = mlp(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Test Accuracy: {100 * correct / total} %')\n",
    "\n",
    "# Evaluate on validation set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in validation_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 784)\n",
    "        outputs = mlp(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Validation Accuracy: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e030751-5fd1-4ba8-98b2-68860fd7d960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading file MNIST\n",
      "Iteration 1, L2 Regularization - Accuracy: 1.00, Loss: 0.00\n",
      "Iteration 2, L2 Regularization - Accuracy: 1.00, Loss: 0.00\n",
      "Iteration 3, L2 Regularization - Accuracy: 1.00, Loss: 0.00\n",
      "Iteration 1, L1 Regularization - Accuracy: 1.00, Loss: 0.16\n",
      "Iteration 2, L1 Regularization - Accuracy: 1.00, Loss: 0.15\n",
      "Iteration 3, L1 Regularization - Accuracy: 1.00, Loss: 0.16\n",
      "Iteration 1, Dropout Regularization - Accuracy: 1.00, Loss: 0.00\n",
      "Iteration 2, Dropout Regularization - Accuracy: 1.00, Loss: 0.00\n",
      "Iteration 3, Dropout Regularization - Accuracy: 1.00, Loss: 0.00\n",
      "Error reading file MNIST\n",
      "Validation Accuracy with L2 Regularization: 1.00\n",
      "Validation Accuracy with L1 Regularization: 1.00\n",
      "Validation Accuracy with Dropout Regularization: 1.00\n",
      "Error reading file evaluation_results_l2.txt\n",
      "Error reading file MNIST\n",
      "Error reading file final_model_l2.h5\n",
      "Error reading file evaluation_results_dropout.txt\n",
      "Error reading file final_model_dropout.h5\n",
      "Error reading file final_model_l1.h5\n",
      "Error reading file evaluation_results_l1.txt\n",
      "Test Accuracy with L2 Regularization: 1.00\n",
      "Test Accuracy with L1 Regularization: 1.00\n",
      "Test Accuracy with Dropout Regularization: 1.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Load the training data from the folder\n",
    "train_folder = '/home/vaishnavi-moutam/Downloads/final/Training1'\n",
    "X_train = []\n",
    "y_train = []\n",
    "for file in os.listdir(train_folder):\n",
    "    img = cv2.imread(os.path.join(train_folder, file))\n",
    "    if img is not None:  # Check if image is not empty\n",
    "        img = cv2.resize(img, (32, 32))  # Resize the image\n",
    "        img = img.flatten()  # Flatten the image into a 1D array\n",
    "        X_train.append(img)  # Append the flattened image to the list\n",
    "        y_train.append(1)  # or 0, depending on the class label\n",
    "    else:\n",
    "        print(f\"Error reading file {file}\")  # Print an error message if the image is empty\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "X_train = X_train / 255.0\n",
    "\n",
    "# Initialize the weights and bias for L2 regularization\n",
    "weights_l2 = np.random.rand(X_train.shape[1])\n",
    "bias_l2 = 0\n",
    "\n",
    "# Initialize the weights and bias for L1 regularization\n",
    "weights_l1 = np.random.rand(X_train.shape[1])\n",
    "bias_l1 = 0\n",
    "\n",
    "# Initialize the weights and bias for Dropout regularization\n",
    "weights_dropout = np.random.rand(X_train.shape[1])\n",
    "bias_dropout = 0\n",
    "\n",
    "# Set the learning rate, regularization parameters, and number of iterations\n",
    "learning_rate = 0.01\n",
    "reg_param_l2 = 0.1  # L2 regularization parameter\n",
    "reg_param_l1 = 0.01  # L1 regularization parameter\n",
    "dropout_rate = 0.2  # Dropout rate\n",
    "n_iters = 3\n",
    "\n",
    "# Training loop with L2 regularization\n",
    "for iter in range(n_iters):\n",
    "    for i, x_i in enumerate(X_train):\n",
    "        # Calculate the predicted output with L2 regularization\n",
    "        predicted_l2 = np.where(np.dot(x_i, weights_l2) + bias_l2 >= 0, 1, -1)\n",
    "        \n",
    "        # Update the weights and bias with L2 regularization\n",
    "        update_l2 = learning_rate * (y_train[i] - predicted_l2)\n",
    "        weights_update_l2 = update_l2 * x_i - reg_param_l2 * weights_l2  # L2 regularization update\n",
    "        weights_l2 += weights_update_l2\n",
    "        bias_l2 += update_l2\n",
    "\n",
    "    # Calculate the accuracy and loss with L2 regularization\n",
    "    predictions_l2 = np.where(np.dot(X_train, weights_l2) + bias_l2 >= 0, 1, -1)\n",
    "    accuracy_l2 = np.mean(predictions_l2 == y_train)\n",
    "    loss_l2 = np.mean((predictions_l2 - y_train) ** 2) + 0.5 * reg_param_l2 * np.sum(weights_l2 ** 2)\n",
    "    \n",
    "    # Print the accuracy and loss with L2 regularization\n",
    "    print(f'Iteration {iter+1}, L2 Regularization - Accuracy: {accuracy_l2:.2f}, Loss: {loss_l2:.2f}')\n",
    "\n",
    "# Save the trained model with L2 regularization\n",
    "np.save('weights_l2.npy', weights_l2)\n",
    "np.save('bias_l2.npy', bias_l2)\n",
    "\n",
    "# Training loop with L1 regularization\n",
    "for iter in range(n_iters):\n",
    "    for i, x_i in enumerate(X_train):\n",
    "        # Calculate the predicted output with L1 regularization\n",
    "        predicted_l1 = np.where(np.dot(x_i, weights_l1) + bias_l1 >= 0, 1, -1)\n",
    "        \n",
    "        # Update the weights and bias with L1 regularization\n",
    "        update_l1 = learning_rate * (y_train[i] - predicted_l1)\n",
    "        weights_update_l1 = update_l1 * x_i - reg_param_l1 * np.sign(weights_l1)  # L1 regularization update\n",
    "        weights_l1 += weights_update_l1\n",
    "        bias_l1 += update_l1\n",
    "\n",
    "    # Calculate the accuracy and loss with L1 regularization\n",
    "    predictions_l1 = np.where(np.dot(X_train, weights_l1) + bias_l1 >= 0, 1, -1)\n",
    "    accuracy_l1 = np.mean(predictions_l1 == y_train)\n",
    "    loss_l1 = np.mean((predictions_l1 - y_train) ** 2) + reg_param_l1 * np.sum(np.abs(weights_l1))\n",
    "    \n",
    "    # Print the accuracy and loss with L1 regularization\n",
    "    print(f'Iteration {iter+1}, L1 Regularization - Accuracy: {accuracy_l1:.2f}, Loss: {loss_l1:.2f}')\n",
    "\n",
    "# Save the trained model with L1 regularization\n",
    "np.save('weights_l1.npy', weights_l1)\n",
    "np.save('bias_l1.npy', bias_l1)\n",
    "\n",
    "# Training loop with Dropout regularization\n",
    "for iter in range(n_iters):\n",
    "    for i, x_i in enumerate(X_train):\n",
    "        # Apply dropout during training\n",
    "        mask = np.random.binomial(1, 1 - dropout_rate, size=weights_dropout.shape)\n",
    "        x_i_dropout = x_i * mask / (1 - dropout_rate)\n",
    "        \n",
    "        # Calculate the predicted output with Dropout regularization\n",
    "        predicted_dropout = np.where(np.dot(x_i_dropout, weights_dropout) + bias_dropout >= 0, 1, -1)\n",
    "        \n",
    "        # Update the weights and bias without regularization (dropout already applied)\n",
    "        update_dropout = learning_rate * (y_train[i] - predicted_dropout)\n",
    "        weights_update_dropout = update_dropout * x_i_dropout\n",
    "        weights_dropout += weights_update_dropout\n",
    "        bias_dropout += update_dropout\n",
    "\n",
    "    # Calculate the accuracy and loss with Dropout regularization\n",
    "    predictions_dropout = np.where(np.dot(X_train, weights_dropout) + bias_dropout >= 0, 1, -1)\n",
    "    accuracy_dropout = np.mean(predictions_dropout == y_train)\n",
    "    loss_dropout = np.mean((predictions_dropout - y_train) ** 2)\n",
    "    \n",
    "    # Print the accuracy and loss with Dropout regularization\n",
    "    print(f'Iteration {iter+1}, Dropout Regularization - Accuracy: {accuracy_dropout:.2f}, Loss: {loss_dropout:.2f}')\n",
    "\n",
    "# Save the trained model with Dropout regularization\n",
    "np.save('weights_dropout.npy', weights_dropout)\n",
    "np.save('bias_dropout.npy', bias_dropout)\n",
    "\n",
    "# Load the validation data from the folder\n",
    "val_folder = '/home/vaishnavi-moutam/Downloads/final/Validation1'\n",
    "X_val = []\n",
    "y_val = []\n",
    "for file in os.listdir(val_folder):\n",
    "    img = cv2.imread(os.path.join(val_folder, file))\n",
    "    if img is not None:  # Check if image is not empty\n",
    "        img = cv2.resize(img, (32, 32))  # Resize the image\n",
    "        img = img.flatten()  # Flatten the image into a 1D array\n",
    "        X_val.append(img)  # Append the flattened image to the list\n",
    "        y_val.append(1)  # or 0, depending on the class label\n",
    "    else:\n",
    "        print(f\"Error reading file {file}\")  # Print an error message if the image is empty\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "X_val = X_val / 255.0\n",
    "\n",
    "# Load the trained models with L1, L2, and Dropout regularization\n",
    "weights_l2 = np.load('weights_l2.npy')\n",
    "bias_l2 = np.load('bias_l2.npy')\n",
    "\n",
    "weights_l1 = np.load('weights_l1.npy')\n",
    "bias_l1 = np.load('bias_l1.npy')\n",
    "\n",
    "weights_dropout = np.load('weights_dropout.npy')\n",
    "bias_dropout = np.load('bias_dropout.npy')\n",
    "\n",
    "# Evaluate the validation set accuracy for each regularization technique\n",
    "predictions_l2 = np.where(np.dot(X_val, weights_l2) + bias_l2 >= 0, 1, -1)\n",
    "accuracy_val_l2 = np.mean(predictions_l2 == y_val)\n",
    "print(f'Validation Accuracy with L2 Regularization: {accuracy_val_l2:.2f}')\n",
    "\n",
    "predictions_l1 = np.where(np.dot(X_val, weights_l1) + bias_l1 >= 0, 1, -1)\n",
    "accuracy_val_l1 = np.mean(predictions_l1 == y_val)\n",
    "print(f'Validation Accuracy with L1 Regularization: {accuracy_val_l1:.2f}')\n",
    "\n",
    "predictions_dropout = np.where(np.dot(X_val, weights_dropout) + bias_dropout >= 0, 1, -1)\n",
    "accuracy_val_dropout = np.mean(predictions_dropout == y_val)\n",
    "print(f'Validation Accuracy with Dropout Regularization: {accuracy_val_dropout:.2f}')\n",
    "\n",
    "# Load the testing data from the folder\n",
    "test_folder = '/home/vaishnavi-moutam/Downloads/final/Testing1'\n",
    "X_test = []\n",
    "y_test = []\n",
    "for file in os.listdir(test_folder):\n",
    "    img = cv2.imread(os.path.join(test_folder, file))\n",
    "    if img is not None:  # Check if image is not empty\n",
    "        img = cv2.resize(img, (32, 32))  # Resize the image\n",
    "        img = img.flatten()  # Flatten the image into a 1D array\n",
    "        X_test.append(img)  # Append the flattened image to the list\n",
    "        y_test.append(1)  # or 0, depending on the class label\n",
    "    else:\n",
    "        print(f\"Error reading file {file}\")  # Print an error message if the image is empty\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Evaluate the test set accuracy for each regularization technique\n",
    "predictions_l2_test = np.where(np.dot(X_test, weights_l2) + bias_l2 >= 0, 1, -1)\n",
    "accuracy_test_l2 = np.mean(predictions_l2_test == y_test)\n",
    "print(f'Test Accuracy with L2 Regularization: {accuracy_test_l2:.2f}')\n",
    "\n",
    "predictions_l1_test = np.where(np.dot(X_test, weights_l1) + bias_l1 >= 0, 1, -1)\n",
    "accuracy_test_l1 = np.mean(predictions_l1_test == y_test)\n",
    "print(f'Test Accuracy with L1 Regularization: {accuracy_test_l1:.2f}')\n",
    "\n",
    "predictions_dropout_test = np.where(np.dot(X_test, weights_dropout) + bias_dropout >= 0, 1, -1)\n",
    "accuracy_test_dropout = np.mean(predictions_dropout_test == y_test)\n",
    "print(f'Test Accuracy with Dropout Regularization: {accuracy_test_dropout:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2587f4d3-6aad-4bf0-890b-542b6e2d357a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping /home/vaishnavi-moutam/Downloads/final/Training1/MNIST/raw/t10k-images-idx3-ubyte: cannot identify image file '/home/vaishnavi-moutam/Downloads/final/Training1/MNIST/raw/t10k-images-idx3-ubyte'\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Training1/MNIST/raw/t10k-images-idx3-ubyte.gz: cannot identify image file '/home/vaishnavi-moutam/Downloads/final/Training1/MNIST/raw/t10k-images-idx3-ubyte.gz'\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Training1/MNIST/raw/t10k-labels-idx1-ubyte: cannot identify image file '/home/vaishnavi-moutam/Downloads/final/Training1/MNIST/raw/t10k-labels-idx1-ubyte'\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Training1/MNIST/raw/t10k-labels-idx1-ubyte.gz: cannot identify image file '/home/vaishnavi-moutam/Downloads/final/Training1/MNIST/raw/t10k-labels-idx1-ubyte.gz'\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Training1/MNIST/raw/train-images-idx3-ubyte: cannot identify image file '/home/vaishnavi-moutam/Downloads/final/Training1/MNIST/raw/train-images-idx3-ubyte'\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Training1/MNIST/raw/train-images-idx3-ubyte.gz: cannot identify image file '/home/vaishnavi-moutam/Downloads/final/Training1/MNIST/raw/train-images-idx3-ubyte.gz'\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Training1/MNIST/raw/train-labels-idx1-ubyte: cannot identify image file '/home/vaishnavi-moutam/Downloads/final/Training1/MNIST/raw/train-labels-idx1-ubyte'\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Training1/MNIST/raw/train-labels-idx1-ubyte.gz: cannot identify image file '/home/vaishnavi-moutam/Downloads/final/Training1/MNIST/raw/train-labels-idx1-ubyte.gz'\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Validation1/MNIST/raw/t10k-images-idx3-ubyte: cannot identify image file '/home/vaishnavi-moutam/Downloads/final/Validation1/MNIST/raw/t10k-images-idx3-ubyte'\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Validation1/MNIST/raw/t10k-images-idx3-ubyte.gz: cannot identify image file '/home/vaishnavi-moutam/Downloads/final/Validation1/MNIST/raw/t10k-images-idx3-ubyte.gz'\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Validation1/MNIST/raw/t10k-labels-idx1-ubyte: cannot identify image file '/home/vaishnavi-moutam/Downloads/final/Validation1/MNIST/raw/t10k-labels-idx1-ubyte'\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Validation1/MNIST/raw/t10k-labels-idx1-ubyte.gz: cannot identify image file '/home/vaishnavi-moutam/Downloads/final/Validation1/MNIST/raw/t10k-labels-idx1-ubyte.gz'\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Validation1/MNIST/raw/train-images-idx3-ubyte: cannot identify image file '/home/vaishnavi-moutam/Downloads/final/Validation1/MNIST/raw/train-images-idx3-ubyte'\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Validation1/MNIST/raw/train-images-idx3-ubyte.gz: cannot identify image file '/home/vaishnavi-moutam/Downloads/final/Validation1/MNIST/raw/train-images-idx3-ubyte.gz'\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Validation1/MNIST/raw/train-labels-idx1-ubyte: cannot identify image file '/home/vaishnavi-moutam/Downloads/final/Validation1/MNIST/raw/train-labels-idx1-ubyte'\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Validation1/MNIST/raw/train-labels-idx1-ubyte.gz: cannot identify image file '/home/vaishnavi-moutam/Downloads/final/Validation1/MNIST/raw/train-labels-idx1-ubyte.gz'\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Testing1/evaluation_results_dropout.txt: cannot identify image file '/home/vaishnavi-moutam/Downloads/final/Testing1/evaluation_results_dropout.txt'\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Testing1/evaluation_results_l1.txt: cannot identify image file '/home/vaishnavi-moutam/Downloads/final/Testing1/evaluation_results_l1.txt'\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Testing1/evaluation_results_l2.txt: cannot identify image file '/home/vaishnavi-moutam/Downloads/final/Testing1/evaluation_results_l2.txt'\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Testing1/final_model_dropout.h5: cannot find loader for this HDF5 file\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Testing1/final_model_l1.h5: cannot find loader for this HDF5 file\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Testing1/final_model_l2.h5: cannot find loader for this HDF5 file\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Testing1/MNIST/raw/t10k-images-idx3-ubyte: cannot identify image file '/home/vaishnavi-moutam/Downloads/final/Testing1/MNIST/raw/t10k-images-idx3-ubyte'\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Testing1/MNIST/raw/t10k-images-idx3-ubyte.gz: cannot identify image file '/home/vaishnavi-moutam/Downloads/final/Testing1/MNIST/raw/t10k-images-idx3-ubyte.gz'\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Testing1/MNIST/raw/t10k-labels-idx1-ubyte: cannot identify image file '/home/vaishnavi-moutam/Downloads/final/Testing1/MNIST/raw/t10k-labels-idx1-ubyte'\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Testing1/MNIST/raw/t10k-labels-idx1-ubyte.gz: cannot identify image file '/home/vaishnavi-moutam/Downloads/final/Testing1/MNIST/raw/t10k-labels-idx1-ubyte.gz'\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Testing1/MNIST/raw/train-images-idx3-ubyte: cannot identify image file '/home/vaishnavi-moutam/Downloads/final/Testing1/MNIST/raw/train-images-idx3-ubyte'\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Testing1/MNIST/raw/train-images-idx3-ubyte.gz: cannot identify image file '/home/vaishnavi-moutam/Downloads/final/Testing1/MNIST/raw/train-images-idx3-ubyte.gz'\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Testing1/MNIST/raw/train-labels-idx1-ubyte: cannot identify image file '/home/vaishnavi-moutam/Downloads/final/Testing1/MNIST/raw/train-labels-idx1-ubyte'\n",
      "Skipping /home/vaishnavi-moutam/Downloads/final/Testing1/MNIST/raw/train-labels-idx1-ubyte.gz: cannot identify image file '/home/vaishnavi-moutam/Downloads/final/Testing1/MNIST/raw/train-labels-idx1-ubyte.gz'\n",
      "Training with Adam optimizer:\n",
      "Epoch [1/3], Loss: 0.5200, Train Acc: 1.0000, Val Acc: 1.0000\n",
      "Epoch [2/3], Loss: 0.0016, Train Acc: 1.0000, Val Acc: 1.0000\n",
      "Epoch [3/3], Loss: 0.0002, Train Acc: 1.0000, Val Acc: 1.0000\n",
      "Testing Accuracy with Adam optimizer: 1.0000\n",
      "\n",
      "Training with SGD optimizer:\n",
      "Epoch [1/3], Loss: 0.0001, Train Acc: 1.0000, Val Acc: 1.0000\n",
      "Epoch [2/3], Loss: 0.0001, Train Acc: 1.0000, Val Acc: 1.0000\n",
      "Epoch [3/3], Loss: 0.0001, Train Acc: 1.0000, Val Acc: 1.0000\n",
      "Testing Accuracy with SGD optimizer: 1.0000\n",
      "\n",
      "Training with RMSprop optimizer:\n",
      "Epoch [1/3], Loss: 0.0000, Train Acc: 1.0000, Val Acc: 1.0000\n",
      "Epoch [2/3], Loss: 0.0000, Train Acc: 1.0000, Val Acc: 1.0000\n",
      "Epoch [3/3], Loss: 0.0000, Train Acc: 1.0000, Val Acc: 1.0000\n",
      "Testing Accuracy with RMSprop optimizer: 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Compare performance with various optimization algorithms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Define transformations\n",
    "transform = Compose([\n",
    "    Resize((28, 28)),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "# Define data paths\n",
    "training_path = \"/home/vaishnavi-moutam/Downloads/final/Training1\"\n",
    "validation_path = \"/home/vaishnavi-moutam/Downloads/final/Validation1\"\n",
    "testing_path = \"/home/vaishnavi-moutam/Downloads/final/Testing1\"\n",
    "\n",
    "# Load training data\n",
    "train_samples = []\n",
    "for root, _, fnames in sorted(os.walk(training_path)):\n",
    "    for fname in sorted(fnames):\n",
    "        path = os.path.join(root, fname)\n",
    "        try:\n",
    "            img = Image.open(path).convert('L')\n",
    "            train_samples.append((transform(img), 0))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {path}: {str(e)}\")\n",
    "train_loader = DataLoader(train_samples, batch_size=32, shuffle=True)\n",
    "\n",
    "# Load validation data\n",
    "val_samples = []\n",
    "for root, _, fnames in sorted(os.walk(validation_path)):\n",
    "    for fname in sorted(fnames):\n",
    "        path = os.path.join(root, fname)\n",
    "        try:\n",
    "            img = Image.open(path).convert('L')\n",
    "            val_samples.append((transform(img), 0))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {path}: {str(e)}\")\n",
    "val_loader = DataLoader(val_samples, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load testing data\n",
    "test_samples = []\n",
    "for root, _, fnames in sorted(os.walk(testing_path)):\n",
    "    for fname in sorted(fnames):\n",
    "        path = os.path.join(root, fname)\n",
    "        try:\n",
    "            img = Image.open(path).convert('L')\n",
    "            test_samples.append((transform(img), 0))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {path}: {str(e)}\")\n",
    "test_loader = DataLoader(test_samples, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define model architecture\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28 * 28, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10)\n",
    ")\n",
    "\n",
    "# Define optimizers to compare\n",
    "optimizers = {\n",
    "    'Adam': optim.Adam(model.parameters(), lr=0.001),\n",
    "    'SGD': optim.SGD(model.parameters(), lr=0.001, momentum=0.9),\n",
    "    'RMSprop': optim.RMSprop(model.parameters(), lr=0.001)\n",
    "}\n",
    "\n",
    "# Training and evaluation loop for each optimizer\n",
    "num_epochs = 3\n",
    "for optimizer_name, optimizer in optimizers.items():\n",
    "    print(f\"Training with {optimizer_name} optimizer:\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in train_loader:\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # Calculate validation accuracy\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        val_acc = correct / total\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    test_acc = correct / total\n",
    "    print(f\"Testing Accuracy with {optimizer_name} optimizer: {test_acc:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24338ac5-b968-4664-a0cf-8f59b3038087",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
